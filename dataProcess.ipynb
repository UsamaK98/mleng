{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files in: C://Users//Usama.Khatab//Projects//Software Projects//mleng//project-info//data\n",
      "Creating visualizations for 2025-01-07\n",
      "Creating visualizations for 2024-10-08\n",
      "Creating visualizations for 2024-09-10\n",
      "Creating visualizations for 2024-06-26\n",
      "Processing complete. Results saved to: output\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Enhanced function to extract timestamps, speakers, roles and content\n",
    "def extract_speech_data(text):\n",
    "    # List to store extracted data\n",
    "    data = []\n",
    "    \n",
    "    # Pattern to find timestamps (format: HH:MM)\n",
    "    timestamp_pattern = r'\\n(\\d{1,2}:\\d{2})\\n'\n",
    "    \n",
    "    # Pattern to capture speaker, role in parentheses, and content\n",
    "    speech_pattern = r'(.*?)(?:\\s*\\((.*?)\\))?\\s*:\\s*(.*?)(?=\\n\\n|\\Z)'\n",
    "    \n",
    "    # First, split text by timestamps\n",
    "    timestamp_splits = re.split(timestamp_pattern, text)\n",
    "    \n",
    "    current_timestamp = None\n",
    "    current_text = \"\"\n",
    "    \n",
    "    # Process the split text\n",
    "    for i, segment in enumerate(timestamp_splits):\n",
    "        if i % 2 == 1:  # This is a timestamp\n",
    "            current_timestamp = segment\n",
    "        else:  # This is content after a timestamp or before the first timestamp\n",
    "            current_text = segment\n",
    "            if current_timestamp:  # If we have a timestamp, process the text\n",
    "                # Find all speaker-content pairs in this timestamped section\n",
    "                matches = re.findall(speech_pattern, current_text, re.DOTALL)\n",
    "                \n",
    "                for match in matches:\n",
    "                    speaker = match[0].strip()\n",
    "                    role = match[1].strip() if match[1] else \"\"\n",
    "                    content = match[2].strip()\n",
    "                    data.append((current_timestamp, speaker, role, content))\n",
    "    \n",
    "    # Process any remaining sections without timestamps\n",
    "    if not data:\n",
    "        matches = re.findall(speech_pattern, text, re.DOTALL)\n",
    "        for match in matches:\n",
    "            speaker = match[0].strip()\n",
    "            role = match[1].strip() if match[1] else \"\"\n",
    "            content = match[2].strip()\n",
    "            data.append((\"\", speaker, role, content))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Process all text files in the directory\n",
    "def process_all_files(directory):\n",
    "    all_data = []\n",
    "    file_dates = {}\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            date_match = re.search(r'(\\d{2})_(\\d{2})_(\\d{2})', filename)\n",
    "            \n",
    "            if date_match:\n",
    "                day, month, year = date_match.groups()\n",
    "                date = f\"20{year}-{month}-{day}\"\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                \n",
    "                # Extract speeches with timestamps\n",
    "                data = extract_speech_data(text)\n",
    "                \n",
    "                # Create DataFrame for this file\n",
    "                if data:\n",
    "                    file_df = pd.DataFrame(data, columns=['Timestamp', 'Speaker', 'Role', 'Content'])\n",
    "                    file_df['Date'] = date\n",
    "                    file_df['Filename'] = filename\n",
    "                    \n",
    "                    all_data.append(file_df)\n",
    "                    file_dates[filename] = date\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        return combined_df, file_dates\n",
    "    \n",
    "    return pd.DataFrame(), {}\n",
    "\n",
    "# Add these visualization functions to your existing code\n",
    "\n",
    "def get_extended_stopwords():\n",
    "    \"\"\"Custom stopwords for parliamentary debates\"\"\"\n",
    "    base_stopwords = set(STOPWORDS)\n",
    "    custom_words = {\n",
    "        'will', 'one', 'whether', 'said', 'get', 'also', 'make', 'well', 'say',\n",
    "        'thank', 'think', 'way', 'come', 'right', 'know', 'take', 'see', 'going',\n",
    "        'would', 'could', 'should', 'may', 'might', 'must', 'shall', 'can',\n",
    "        'just', 'now', 'look', 'want', 'back', 'much', 'many', 'lot', 'thing',\n",
    "        'next', 'made', 'like', 'good', 'set', 'put', 'year', 'day', 'time',\n",
    "        'use', 'used', 'using', 'new', 'old', 'first', 'last', 'able', 'need',\n",
    "        'point', 'every', 'across', 'example', 'really', 'quite', 'mean', 'within'\n",
    "    }\n",
    "    return base_stopwords.union(custom_words)\n",
    "\n",
    "def generate_word_cloud(text, title, output_path):\n",
    "    \"\"\"Generate filtered word cloud visualization\"\"\"\n",
    "    wordcloud = WordCloud(\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        background_color='white',\n",
    "        stopwords=get_extended_stopwords(),\n",
    "        max_words=150,\n",
    "        collocations=False\n",
    "    ).generate(text)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, pad=20)\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def generate_speaker_network(date_df, output_path):\n",
    "    \"\"\"Create improved speaker interaction network\"\"\"\n",
    "    interactions = defaultdict(int)\n",
    "    speakers = date_df['Speaker'].tolist()\n",
    "    \n",
    "    for i in range(1, len(speakers)):\n",
    "        prev_speaker = speakers[i-1]\n",
    "        current_speaker = speakers[i]\n",
    "        if prev_speaker != current_speaker:\n",
    "            interactions[(prev_speaker, current_speaker)] += 1\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for (source, target), weight in interactions.items():\n",
    "        G.add_edge(source, target, weight=weight)\n",
    "\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    pos = nx.spring_layout(G, k=0.5, seed=42)\n",
    "    \n",
    "    # Node sizing based on degree\n",
    "    node_sizes = [2000 + G.degree(speaker)*300 for speaker in G.nodes()]\n",
    "    \n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos,\n",
    "        node_size=node_sizes,\n",
    "        node_color='lightblue',\n",
    "        alpha=0.9\n",
    "    )\n",
    "    \n",
    "    # Edge styling\n",
    "    edge_weights = [G[u][v]['weight']*2 for u,v in G.edges()]\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos,\n",
    "        width=edge_weights,\n",
    "        edge_color='gray',\n",
    "        alpha=0.7,\n",
    "        arrowsize=20\n",
    "    )\n",
    "    \n",
    "    # Labels with roles\n",
    "    roles = date_df.set_index('Speaker')['Role'].to_dict()\n",
    "    labels = {node: f\"{node}\\n({roles.get(node, '')})\" for node in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=9, font_family='sans-serif')\n",
    "    \n",
    "    plt.title(\"Speaker Interaction Flow\", pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Update the main processing function\n",
    "def process_parliamentary_minutes(directory, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing files in: {directory}\")\n",
    "    df, file_dates = process_all_files(directory)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No valid files found.\")\n",
    "        return\n",
    "\n",
    "    # Sort and save data\n",
    "    df = df.sort_values(['Date', 'Timestamp'])\n",
    "    df.to_csv(os.path.join(output_dir, \"parliamentary_minutes.csv\"), index=False)\n",
    "    \n",
    "    # Process each date separately for visualizations\n",
    "    for filename, date in file_dates.items():\n",
    "        date_str = date.replace('-', '_')\n",
    "        date_df = df[df['Date'] == date]\n",
    "        \n",
    "        print(f\"Creating visualizations for {date}\")\n",
    "        \n",
    "        # Word Cloud\n",
    "        generate_word_cloud(\n",
    "            ' '.join(date_df['Content']),\n",
    "            f\"Key Topics - {date}\",\n",
    "            os.path.join(output_dir, f\"wordcloud_{date_str}.png\")\n",
    "        )\n",
    "        \n",
    "        # Speaker Frequency\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        date_df['Speaker'].value_counts().plot(kind='barh', color='skyblue')\n",
    "        plt.title(f\"Speaking Frequency - {date}\")\n",
    "        plt.xlabel('Number of Contributions')\n",
    "        plt.ylabel('Speaker')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"speaker_freq_{date_str}.png\"), dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Interaction Network\n",
    "        generate_speaker_network(\n",
    "            date_df,\n",
    "            os.path.join(output_dir, f\"interaction_network_{date_str}.png\")\n",
    "        )\n",
    "\n",
    "    print(f\"Processing complete. Results saved to: {output_dir}\")\n",
    "    return df, extract_speakers_and_roles(df)\n",
    "\n",
    "\n",
    "# Extract speakers and their roles function\n",
    "def extract_speakers_and_roles(df):\n",
    "    speakers_data = []\n",
    "    \n",
    "    for speaker in df['Speaker'].unique():\n",
    "        speaker_df = df[df['Speaker'] == speaker]\n",
    "        roles = speaker_df['Role'].unique()\n",
    "        role = roles[0] if roles[0] and roles[0] != \"\" else \"Unknown\"\n",
    "        \n",
    "        # Calculate speaking statistics\n",
    "        word_count = speaker_df['Content'].str.split().str.len().sum()\n",
    "        contribution_count = len(speaker_df)\n",
    "        avg_words_per_contribution = word_count / contribution_count if contribution_count > 0 else 0\n",
    "        \n",
    "        speakers_data.append({\n",
    "            'Speaker': speaker,\n",
    "            'Role/Organization': role,\n",
    "            'Number_of_Contributions': contribution_count,\n",
    "            'Total_Words': word_count,\n",
    "            'Average_Words_Per_Contribution': avg_words_per_contribution\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(speakers_data).sort_values('Number_of_Contributions', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "input_dir = \"C://Users//Usama.Khatab//Projects//Software Projects//mleng//project-info//data\"  # Change to your directory with text files\n",
    "output_dir = \"output\"  # Change to your desired output directory\n",
    "df, speakers_df = process_parliamentary_minutes(input_dir, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
